{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfa247b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T19:22:19.893985Z",
     "start_time": "2022-11-18T19:21:16.347708Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: <object object at 0x0000021A5AAA54C0>\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow\n",
    "import warnings\n",
    "\n",
    "\n",
    "%matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcc42c3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T19:22:20.177140Z",
     "start_time": "2022-11-18T19:22:19.898813Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Gustavo M\n",
      "\n",
      "json       : 2.0.9\n",
      "numpy      : 1.21.0\n",
      "torch      : 1.12.0+cu116\n",
      "tensorflow : 2.9.1\n",
      "torchvision: 0.13.0\n",
      "matplotlib : 3.5.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -a \"Gustavo M\" --iversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70972758",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T19:25:10.884952Z",
     "start_time": "2022-11-18T19:25:10.865756Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available. Training will run on GPU.\n"
     ]
    }
   ],
   "source": [
    "# Checking if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA not available. Training will run on CPU')\n",
    "else:\n",
    "    print('CUDA available. Training will run on GPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76f4e2e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T19:28:06.271139Z",
     "start_time": "2022-11-18T19:27:47.898819Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfa0075d0361447cad3abfaeabb04664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\cifar-10-python.tar.gz to data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Dataset loading\n",
    "\n",
    "transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                transforms.RandomRotation(10),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "train_data = datasets.CIFAR10('data', \n",
    "                                train = True,\n",
    "                                download = True, \n",
    "                                transform = transform)\n",
    "\n",
    "test_data = datasets.CIFAR10('data', \n",
    "                               train = False,\n",
    "                               download = True, \n",
    "                               transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c07eb67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T19:58:22.313504Z",
     "start_time": "2022-11-18T19:58:22.283988Z"
    }
   },
   "outputs": [],
   "source": [
    "# Splitting data\n",
    "\n",
    "num_train_samples = len(train_data)\n",
    "\n",
    "indices = list(range(num_train_samples))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "valid_size = 0.2\n",
    "\n",
    "\n",
    "split = int(np.floor(valid_size * num_train_samples))\n",
    "idx_train, idx_valid = indices[split:], indices[:split]\n",
    "\n",
    "train_samples = SubsetRandomSampler(idx_train)\n",
    "valid_samples = SubsetRandomSampler(idx_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58564407",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T19:58:30.296300Z",
     "start_time": "2022-11-18T19:58:30.282530Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data Loaders\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 20\n",
    "\n",
    "loader_train = torch.utils.data.DataLoader(train_data, \n",
    "                                            batch_size = batch_size, \n",
    "                                            sampler = train_samples, \n",
    "                                            num_workers = num_workers)\n",
    "\n",
    "\n",
    "loader_valid = torch.utils.data.DataLoader(train_data, \n",
    "                                           batch_size = batch_size, \n",
    "                                           sampler = valid_samples, \n",
    "                                           num_workers = num_workers)\n",
    "\n",
    "\n",
    "loader_test = torch.utils.data.DataLoader(test_data, \n",
    "                                           batch_size = batch_size, \n",
    "                                           num_workers = num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ab3106c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T19:58:31.561382Z",
     "start_time": "2022-11-18T19:58:31.548222Z"
    }
   },
   "outputs": [],
   "source": [
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e7ea179",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T19:58:32.936876Z",
     "start_time": "2022-11-18T19:58:32.911053Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convolutional Neural Network Model\n",
    "class CNNmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CNNmodel, self).__init__()\n",
    "        \n",
    "        # Input layers\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding = 1)\n",
    "        \n",
    "        # Hidden Layer \n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding = 1)\n",
    "        \n",
    "        # Hidden Layer \n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, padding = 1)\n",
    "        \n",
    "        # Max Pooling\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Fully connected 1\n",
    "        self.fc1 = nn.Linear(64 * 4 * 4, 500)\n",
    "        \n",
    "        # Fully connected 2\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "        \n",
    "        # Dropout (Regularization)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    # Forward\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Relu Activation\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        \n",
    "        # Flattening\n",
    "        x = x.view(-1, 64 * 4 * 4)\n",
    "        \n",
    "        # Add one dropout for regularization\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Add first hidden layer with Relu activation\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        # Another dropout\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Add second hidden layer with Relu activation\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2a9fb48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T19:58:33.990560Z",
     "start_time": "2022-11-18T19:58:33.960433Z"
    }
   },
   "outputs": [],
   "source": [
    "model = CNNmodel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3b5dd57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T19:58:35.383254Z",
     "start_time": "2022-11-18T19:58:34.566194Z"
    }
   },
   "outputs": [],
   "source": [
    "# Moving model to GPU if available\n",
    "if train_on_gpu:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "adb549ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T19:58:35.398404Z",
     "start_time": "2022-11-18T19:58:35.385543Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loss function cross-entropy\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "12ed28d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T19:58:36.359885Z",
     "start_time": "2022-11-18T19:58:36.345697Z"
    }
   },
   "outputs": [],
   "source": [
    "# SGD\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c1cf5765",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T20:25:25.376102Z",
     "start_time": "2022-11-18T20:05:37.612782Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1 \tTraining Loss: 1.465765 \tValidation Loss: 0.325176\n",
      "Validation Loss deacresed (inf --> 0.325176). Saving model.\n",
      "\n",
      "Epoch: 2 \tTraining Loss: 1.305327 \tValidation Loss: 0.297695\n",
      "Validation Loss deacresed (0.325176 --> 0.297695). Saving model.\n",
      "\n",
      "Epoch: 3 \tTraining Loss: 1.213470 \tValidation Loss: 0.280039\n",
      "Validation Loss deacresed (0.297695 --> 0.280039). Saving model.\n",
      "\n",
      "Epoch: 4 \tTraining Loss: 1.151303 \tValidation Loss: 0.265592\n",
      "Validation Loss deacresed (0.280039 --> 0.265592). Saving model.\n",
      "\n",
      "Epoch: 5 \tTraining Loss: 1.100394 \tValidation Loss: 0.251384\n",
      "Validation Loss deacresed (0.265592 --> 0.251384). Saving model.\n",
      "\n",
      "Epoch: 6 \tTraining Loss: 1.063094 \tValidation Loss: 0.243460\n",
      "Validation Loss deacresed (0.251384 --> 0.243460). Saving model.\n",
      "\n",
      "Epoch: 7 \tTraining Loss: 1.023097 \tValidation Loss: 0.234593\n",
      "Validation Loss deacresed (0.243460 --> 0.234593). Saving model.\n",
      "\n",
      "Epoch: 8 \tTraining Loss: 0.996121 \tValidation Loss: 0.225566\n",
      "Validation Loss deacresed (0.234593 --> 0.225566). Saving model.\n",
      "\n",
      "Epoch: 9 \tTraining Loss: 0.964674 \tValidation Loss: 0.218148\n",
      "Validation Loss deacresed (0.225566 --> 0.218148). Saving model.\n",
      "\n",
      "Epoch: 10 \tTraining Loss: 0.941962 \tValidation Loss: 0.213824\n",
      "Validation Loss deacresed (0.218148 --> 0.213824). Saving model.\n",
      "\n",
      "Epoch: 11 \tTraining Loss: 0.917115 \tValidation Loss: 0.211668\n",
      "Validation Loss deacresed (0.213824 --> 0.211668). Saving model.\n",
      "\n",
      "Epoch: 12 \tTraining Loss: 0.897613 \tValidation Loss: 0.204285\n",
      "Validation Loss deacresed (0.211668 --> 0.204285). Saving model.\n",
      "\n",
      "Epoch: 13 \tTraining Loss: 0.879111 \tValidation Loss: 0.195302\n",
      "Validation Loss deacresed (0.204285 --> 0.195302). Saving model.\n",
      "\n",
      "Epoch: 14 \tTraining Loss: 0.862637 \tValidation Loss: 0.195320\n",
      "\n",
      "Epoch: 15 \tTraining Loss: 0.846295 \tValidation Loss: 0.190186\n",
      "Validation Loss deacresed (0.195302 --> 0.190186). Saving model.\n",
      "\n",
      "Epoch: 16 \tTraining Loss: 0.831577 \tValidation Loss: 0.191689\n",
      "\n",
      "Epoch: 17 \tTraining Loss: 0.817912 \tValidation Loss: 0.184190\n",
      "Validation Loss deacresed (0.190186 --> 0.184190). Saving model.\n",
      "\n",
      "Epoch: 18 \tTraining Loss: 0.809322 \tValidation Loss: 0.182723\n",
      "Validation Loss deacresed (0.184190 --> 0.182723). Saving model.\n",
      "\n",
      "Epoch: 19 \tTraining Loss: 0.793909 \tValidation Loss: 0.180118\n",
      "Validation Loss deacresed (0.182723 --> 0.180118). Saving model.\n",
      "\n",
      "Epoch: 20 \tTraining Loss: 0.780559 \tValidation Loss: 0.177188\n",
      "Validation Loss deacresed (0.180118 --> 0.177188). Saving model.\n",
      "\n",
      "Epoch: 21 \tTraining Loss: 0.774094 \tValidation Loss: 0.176332\n",
      "Validation Loss deacresed (0.177188 --> 0.176332). Saving model.\n",
      "\n",
      "Epoch: 22 \tTraining Loss: 0.767385 \tValidation Loss: 0.172024\n",
      "Validation Loss deacresed (0.176332 --> 0.172024). Saving model.\n",
      "\n",
      "Epoch: 23 \tTraining Loss: 0.753727 \tValidation Loss: 0.170672\n",
      "Validation Loss deacresed (0.172024 --> 0.170672). Saving model.\n",
      "\n",
      "Epoch: 24 \tTraining Loss: 0.750713 \tValidation Loss: 0.167961\n",
      "Validation Loss deacresed (0.170672 --> 0.167961). Saving model.\n",
      "\n",
      "Epoch: 25 \tTraining Loss: 0.741574 \tValidation Loss: 0.167501\n",
      "Validation Loss deacresed (0.167961 --> 0.167501). Saving model.\n",
      "\n",
      "Epoch: 26 \tTraining Loss: 0.734704 \tValidation Loss: 0.166242\n",
      "Validation Loss deacresed (0.167501 --> 0.166242). Saving model.\n",
      "\n",
      "Epoch: 27 \tTraining Loss: 0.725701 \tValidation Loss: 0.169246\n",
      "\n",
      "Epoch: 28 \tTraining Loss: 0.719309 \tValidation Loss: 0.165459\n",
      "Validation Loss deacresed (0.166242 --> 0.165459). Saving model.\n",
      "\n",
      "Epoch: 29 \tTraining Loss: 0.714425 \tValidation Loss: 0.159240\n",
      "Validation Loss deacresed (0.165459 --> 0.159240). Saving model.\n",
      "\n",
      "Epoch: 30 \tTraining Loss: 0.708175 \tValidation Loss: 0.157465\n",
      "Validation Loss deacresed (0.159240 --> 0.157465). Saving model.\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "error_valid_min = np.Inf \n",
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "\n",
    "    # monitoring error on train and validation\n",
    "    error_train = 0.0\n",
    "    error_valid = 0.0\n",
    "    \n",
    "    # Inicia o treinamento do modelo\n",
    "    model.train()\n",
    "    \n",
    "    # Loop batchs train data\n",
    "    for batch_idx, (data, target) in enumerate(loader_train):\n",
    "        \n",
    "        # tensor to GPU\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        \n",
    "        # Clearing gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward\n",
    "        output = model(data)\n",
    "        \n",
    "        # Calculating Loss\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # Backward: Calculating Gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Updating error\n",
    "        error_train += loss.item() * data.size(0)\n",
    "        \n",
    "    # Model evaluation\n",
    "    model.eval()\n",
    "    \n",
    "    # Loop batchs validation\n",
    "    for batch_idx, (data, target) in enumerate(loader_valid):\n",
    "        \n",
    "        # tensors to GPU\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        \n",
    "        # Forward\n",
    "        output = model(data)\n",
    "        \n",
    "        # Calculating loss\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # Updating error\n",
    "        error_valid += loss.item() * data.size(0)\n",
    "    \n",
    "    # Average Error\n",
    "    error_train = error_train / len(loader_train.dataset)\n",
    "    error_valid = error_valid / len(loader_valid.dataset)\n",
    "        \n",
    "    # Print\n",
    "    print('\\nEpoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch, \n",
    "                                                                                         error_train, \n",
    "                                                                                         error_valid))\n",
    "    \n",
    "    # Save the model whenever the validation loss decreases\n",
    "    if error_valid <= error_valid_min:\n",
    "        print('Validation Loss deacresed ({:.6f} --> {:.6f}). Saving model.'.format(error_valid_min,\n",
    "                                                                                                 error_valid))\n",
    "        torch.save(model.state_dict(), 'models/final_model.pt')\n",
    "        error_valid_min = error_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d2b9d344",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T22:16:50.125274Z",
     "start_time": "2022-11-18T22:16:50.096518Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading final model\n",
    "model.load_state_dict(torch.load('models/final_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a9744292",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-18T22:22:03.617985Z",
     "start_time": "2022-11-18T22:21:50.713851Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss on Test: 0.784673\n",
      "\n",
      "Test Acurracy on class airplane: 74% (748/1000)\n",
      "Test Acurracy on class automobile: 83% (834/1000)\n",
      "Test Acurracy on class  bird: 50% (504/1000)\n",
      "Test Acurracy on class   cat: 57% (578/1000)\n",
      "Test Acurracy on class  deer: 70% (703/1000)\n",
      "Test Acurracy on class   dog: 62% (629/1000)\n",
      "Test Acurracy on class  frog: 83% (837/1000)\n",
      "Test Acurracy on class horse: 79% (797/1000)\n",
      "Test Acurracy on class  ship: 82% (828/1000)\n",
      "Test Acurracy on class truck: 83% (834/1000)\n",
      "\n",
      "Test Accuracy (Total): 72% (7292/10000)\n"
     ]
    }
   ],
   "source": [
    "# Evaluating model\n",
    "error_test = 0.0\n",
    "\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Loop batches test\n",
    "for batch_idx, (data, target) in enumerate(loader_test):\n",
    "    \n",
    "    # tensors to GPU\n",
    "    if train_on_gpu:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "    \n",
    "    # Forward\n",
    "    output = model(data)\n",
    "    \n",
    "    # Calculating loss\n",
    "    loss = criterion(output, target)\n",
    "    \n",
    "    # Updating Loss on Test\n",
    "    error_test += loss.item() * data.size(0)\n",
    "    \n",
    "    # Propabilities to prediciton\n",
    "    _, pred = torch.max(output, 1)    \n",
    "    \n",
    "    # Comparing predictions to real target value\n",
    "    correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    \n",
    "    # Calculating precision on each class\n",
    "    for i in range(batch_size):\n",
    "        label = target.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "# Average loss\n",
    "error_test = error_test / len(loader_test.dataset)\n",
    "print('\\nLoss on Test: {:.6f}\\n'.format(error_test))\n",
    "\n",
    "# Accuracy on each class\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Acurracy on class %5s: %2d%% (%2d/%2d)' % (classes[i], \n",
    "                                                             100 * class_correct[i] / class_total[i],\n",
    "                                                             np.sum(class_correct[i]), \n",
    "                                                             np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test accuracy %5s:)' % (classes[i]))\n",
    "\n",
    "# Calcula a acur√°cia total\n",
    "print('\\nTest Accuracy (Total): %2d%% (%2d/%2d)' % (100. * np.sum(class_correct) / np.sum(class_total),\n",
    "                                                        np.sum(class_correct), \n",
    "                                                        np.sum(class_total)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
